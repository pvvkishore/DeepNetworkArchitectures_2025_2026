{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d61ad15-8a03-461c-b778-2b216fcc1a24",
   "metadata": {},
   "source": [
    "# Design, Train and Test a ResNet 18 model from scratch on CIFAR10 dataset. Plot the performance metrics and evaluate the trained model on the test set by visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca0c0e-0391-4696-9667-400f2b588a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/100], Train Loss: 2.6889, Test Loss: 2.1779, Test Acc: 17.52%\n",
      "Epoch [2/100], Train Loss: 2.1230, Test Loss: 2.0414, Test Acc: 23.24%\n",
      "Epoch [3/100], Train Loss: 2.0477, Test Loss: 2.0148, Test Acc: 19.24%\n",
      "Epoch [4/100], Train Loss: 1.9413, Test Loss: 1.8827, Test Acc: 27.68%\n",
      "Epoch [5/100], Train Loss: 1.8430, Test Loss: 1.7864, Test Acc: 33.00%\n",
      "Epoch [6/100], Train Loss: 1.7652, Test Loss: 1.6888, Test Acc: 36.28%\n",
      "Epoch [7/100], Train Loss: 1.6917, Test Loss: 1.7215, Test Acc: 34.04%\n",
      "Epoch [8/100], Train Loss: 1.6128, Test Loss: 1.5891, Test Acc: 40.28%\n",
      "Epoch [9/100], Train Loss: 1.5493, Test Loss: 1.6206, Test Acc: 39.08%\n",
      "Epoch [10/100], Train Loss: 1.5028, Test Loss: 1.5973, Test Acc: 42.00%\n",
      "Epoch [11/100], Train Loss: 1.3934, Test Loss: 1.4885, Test Acc: 44.24%\n",
      "Epoch [12/100], Train Loss: 1.3347, Test Loss: 1.4727, Test Acc: 45.72%\n",
      "Epoch [13/100], Train Loss: 1.2689, Test Loss: 1.4162, Test Acc: 47.36%\n",
      "Epoch [14/100], Train Loss: 1.1883, Test Loss: 1.3832, Test Acc: 50.68%\n",
      "Epoch [15/100], Train Loss: 1.0986, Test Loss: 1.7998, Test Acc: 43.44%\n",
      "Epoch [16/100], Train Loss: 1.0277, Test Loss: 1.4404, Test Acc: 50.12%\n",
      "Epoch [17/100], Train Loss: 0.8994, Test Loss: 1.4749, Test Acc: 50.24%\n",
      "Epoch [18/100], Train Loss: 0.8064, Test Loss: 1.7559, Test Acc: 48.04%\n",
      "Epoch [19/100], Train Loss: 0.6692, Test Loss: 1.7874, Test Acc: 50.00%\n",
      "Epoch [20/100], Train Loss: 0.5767, Test Loss: 1.7451, Test Acc: 51.00%\n",
      "Epoch [21/100], Train Loss: 0.3994, Test Loss: 2.2068, Test Acc: 49.72%\n",
      "Epoch [22/100], Train Loss: 0.3998, Test Loss: 2.1659, Test Acc: 50.60%\n",
      "Epoch [23/100], Train Loss: 0.3117, Test Loss: 2.0231, Test Acc: 53.36%\n",
      "Epoch [24/100], Train Loss: 0.2545, Test Loss: 2.2562, Test Acc: 51.20%\n",
      "Epoch [25/100], Train Loss: 0.1819, Test Loss: 2.3132, Test Acc: 52.52%\n",
      "Epoch [26/100], Train Loss: 0.1290, Test Loss: 2.5616, Test Acc: 52.12%\n",
      "Epoch [27/100], Train Loss: 0.1763, Test Loss: 2.4470, Test Acc: 51.68%\n",
      "Epoch [28/100], Train Loss: 0.1896, Test Loss: 2.6512, Test Acc: 51.76%\n",
      "Epoch [29/100], Train Loss: 0.1280, Test Loss: 2.7264, Test Acc: 51.52%\n",
      "Epoch [30/100], Train Loss: 0.1481, Test Loss: 2.7984, Test Acc: 52.08%\n",
      "Epoch [31/100], Train Loss: 0.2120, Test Loss: 2.5068, Test Acc: 52.48%\n",
      "Epoch [32/100], Train Loss: 0.0812, Test Loss: 2.6797, Test Acc: 53.40%\n",
      "Epoch [33/100], Train Loss: 0.0477, Test Loss: 2.8704, Test Acc: 52.40%\n",
      "Epoch [34/100], Train Loss: 0.0725, Test Loss: 3.2195, Test Acc: 50.60%\n",
      "Epoch [35/100], Train Loss: 0.0648, Test Loss: 3.0163, Test Acc: 52.00%\n"
     ]
    }
   ],
   "source": [
    "# Reimport necessary libraries after execution state reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset and create small subsets 60000 \n",
    "batch_size = 32  # Minimal batch size for memory efficiency default = 32\n",
    "subset_size = 5000  # Use a subset of CIFAR-10 for training\n",
    "\n",
    "# Download CIFAR-10\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "full_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create small subsets for training and testing\n",
    "train_subset, _ = random_split(full_trainset, [subset_size, len(full_trainset) - subset_size])\n",
    "test_subset, _ = random_split(full_testset, [subset_size // 2, len(full_testset) - subset_size // 2])\n",
    "\n",
    "# Create dataloaders\n",
    "trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Create ResNet-18 model\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# Train the model with reduced memory usage\n",
    "num_epochs = 100  # Reduce epochs to prevent memory overload\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_losses.append(test_loss / len(testloader))\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Acc: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save(model.state_dict(), \"resnet18_cifar10_small.pth\")\n",
    "\n",
    "# Plot Training and Testing Loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs. Testing Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Proceed to evaluation separately to optimize memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d663794-8aeb-4802-a310-5679714f00e0",
   "metadata": {},
   "source": [
    "# Increase test accuracy to 90%\n",
    "# Avoid Overfitting\n",
    "# Print Classification report\n",
    "# Visualize the test results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a27bde-6bc3-4929-8a5a-299b1049bceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
